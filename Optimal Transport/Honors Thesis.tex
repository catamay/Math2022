\documentclass[psamsfonts]{amsart}

%-------Packages---------
\usepackage{amssymb,amsfonts}
\usepackage[all,arc]{xy}
\usepackage{enumerate}
\usepackage{mathrsfs}
\usepackage[utf8]{inputenc}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage[shortlabels]{enumitem}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{mdframed}
\usepackage{float}
\usepackage[margin=0.75in]{geometry}
\usepackage{subfigure}

%%% hyperref stuff is taken from AGT style file
\usepackage{hyperref}  
\hypersetup{%
  bookmarksnumbered=true,%
  bookmarks=true,%
  colorlinks=true,%
  linkcolor=blue,%
  citecolor=blue,%
  filecolor=blue,%
  menucolor=blue,%
  pagecolor=blue,%
  urlcolor=blue,%
  pdfnewwindow=true,%
  pdfstartview=FitBH}   
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\ep}{\varepsilon}
\newcommand{\LIM}{\mathop{\textup{LIM}}}
\let\fullref\autoref
%
%  \autoref is very crude.  It uses counters to distinguish environments
%  so that if say {lemma} uses the {theorem} counter, then autrorefs
%  which should come out Lemma X.Y in fact come out Theorem X.Y.  To
%  correct this give each its own counter eg:
%                 \newtheorem{theorem}{Theorem}[section]
%                 \newtheorem{lemma}{Lemma}[section]
%  and then equate the counters by commands like:
%                 \makeatletter
%                   \let\c@lemma\c@theorem
%                  \makeatother
%
%  To work correctly the environment name must have a corrresponding 
%  \XXXautorefname defined.  The following command does the job:
%
\def\makeautorefname#1#2{\expandafter\def\csname#1autorefname\endcsname{#2}}
%
%  Some standard autorefnames.  If the environment name for an autoref 
%  you need is not listed below, add a similar line to your TeX file:
%  
%\makeautorefname{equation}{Equation}%
\def\equationautorefname~#1\null{(#1)\null}
\makeautorefname{footnote}{footnote}%
\makeautorefname{item}{item}%
\makeautorefname{figure}{Figure}%
\makeautorefname{table}{Table}%
\makeautorefname{part}{Part}%
\makeautorefname{appendix}{Appendix}%
\makeautorefname{chapter}{Chapter}%
\makeautorefname{section}{Section}%
\makeautorefname{subsection}{Section}%
\makeautorefname{subsubsection}{Section}%
\makeautorefname{theorem}{Theorem}%
\makeautorefname{thm}{Theorem}%
\makeautorefname{sta}{Statement}%
\makeautorefname{cor}{Corollary}%
\makeautorefname{lem}{Lemma}%
\makeautorefname{prop}{Proposition}%
\makeautorefname{pro}{Property}
\makeautorefname{conj}{Conjecture}%
\makeautorefname{conj}{Convention}%
\makeautorefname{defn}{Definition}%
\makeautorefname{notn}{Notation}
\makeautorefname{notns}{Notations}
\makeautorefname{rem}{Remark}%
\makeautorefname{rems}{Remarks}%
\makeautorefname{quest}{Question}%
\makeautorefname{exmp}{Example}%
\makeautorefname{ax}{Axiom}%
\makeautorefname{claim}{Claim}%
\makeautorefname{ass}{Assumption}%
\makeautorefname{asses}{Assumptions}%
\makeautorefname{con}{Construction}%
\makeautorefname{prob}{Problem}%
\makeautorefname{warn}{Warning}%
\makeautorefname{obs}{Observation}%
\definecolor{problem}{rgb}{0.8,0.8,0.8}
\newcommand{\comp}[2]{
\vspace{0.2in}\begin{mdframed}[
  backgroundcolor=problem,
  userdefinedwidth=10cm,
  align=center,
  skipabove=\topsep,
  skipbelow=\topsep
  ]
  \emph{{#1}:\newline} {#2}
\end{mdframed}}

%
%                  *** End of hyperref stuff ***

%theoremstyle{plain} --- default
\newtheorem{thm}{Theorem}[section]
\newtheorem{sta}{Statement}[section]
\newtheorem{cor}{Corollary}[section]
\newtheorem{prop}{Proposition}[section]
\newtheorem{lem}{Lemma}[section]
\newtheorem{prob}{Problem}[section]
\newtheorem{conj}{Conjecture}[section]

\theoremstyle{definition}
\newtheorem{defn}{Definition}[section]
\newtheorem{conv}{Convention}[section]
\newtheorem{ass}{Assumption}[section]
\newtheorem{asses}{Assumptions}[section]
\newtheorem{ax}{Axiom}[section]
\newtheorem{con}{Construction}[section]
\newtheorem{exmp}{Example}[section]
\newtheorem{notn}{Notation}[section]
\newtheorem{notns}{Notations}[section]
\newtheorem{pro}{Property}[section]
\newtheorem{quest}{Question}[section]
\newtheorem{rem}{Remark}[section]
\newtheorem{rems}{Remarks}[section]
\newtheorem{warn}{Warning}[section]
\newtheorem{sch}{Scholium}[section]
\newtheorem{obs}{Observation}[section]

%%%% hack to get fullref working correctly
\makeatletter
\let\c@obs=\c@thm
\let\c@cor=\c@thm
\let\c@prop=\c@thm
\let\c@lem=\c@thm
\let\c@prob=\c@thm
\let\c@con=\c@thm
\let\c@conj=\c@thm
\let\c@defn=\c@thm
\let\c@notn=\c@thm
\let\c@notns=\c@thm
\let\c@exmp=\c@thm
\let\c@ax=\c@thm
\let\c@pro=\c@thm
\let\c@ass=\c@thm
\let\c@warn=\c@thm
\let\c@rem=\c@thm
\let\c@conv=\c@thm
\let\c@sch=\c@thm
\numberwithin{equation}{section}
\makeatother



\bibliographystyle{plain}

%--------Meta Data: Fill in your info------
\title{Optimal Transport and Applications}

\author{Aidan Copinga}

\date{2/5/2022 DEADLINES: Draft MARCH 28, 2022 and Final version MAY 6, 2022}

\begin{document}

\begin{abstract}

In the 18th century, a problem was created about the best way to transport munitions to each barracks in France. This problem, now called optimal transportation, has shown useful in many fields of mathematics, from PDEs to image processing and machine learning.
This paper introduces the Monge-Kantorovich Problem and its dual problem. Using this, we are able to discuss existence of transport plans and maps of the Monge-Kantorovich problem. Finally, we cover the Wasserstein Space and applications with respect to gradient flows.

\end{abstract}

\maketitle

\tableofcontents

\section*{Acknowledgments}
\section{Formulation of Optimal Transport}
\subsection{Problem}\hfill\\

Start with a simple example where we have a pile of dirt, and a hole to fill completely with the dirt.
To transport the dirt from this pile to the hole, clearly, the volume of the hole and the pile must be the same. As is suggested in \cite{villani}, we normalize this mass to $1$.\newline
To model this rigorously, allow the pile to be space $X$ and the hole to be space $Y$. It's natural to define probability measures $\mu,\nu$ on $X,Y$ respectively.
In other words, for measurable $A\subset X$ and $B\subset Y$, $\mu(A)$ is the volume in the pile $X$ and $\nu(B)$ is the volume to be filled in hole $Y$.\newline
Now, introduce cost function $c: X\times Y \to \mathbb{R}$ such that for points $x\in X,y\in Y$, $c(x,y)$ represents how much energy is exhausted transporting point $x$ in the pile to point $y$ in the hole.\newline
We can now introduce the transportation problem.
%\comp{}{Given $n$ piles of sand and $m$ holes to be filled completely.}
\begin{defn}[Transportation Problem]\label{defn:tp}
Given measure spaces $X,Y$ and $\mu,\nu$ respectively on $X,Y$ and $c: X\times Y\to \mathbb{R}$. The Transportation Problem is realizing the transport plan from Figure \ref{fig:tran} while minimizing $c$.
\end{defn}
\begin{figure}[H]
  \includegraphics[scale=0.3]{transport.jpg}
  \caption{Transportation Problem}
  \label{fig:tran}
\end{figure}

There are two primary formulation for the transportation problem, the Monge and the Kantorovich formulation. The Monge formulation came historically before Kantorovich's, so I will be introducing it first.
\subsection{Monge Formulation}\hfill\\

\autoref{fig:tran} uses the Lagrangian framework in order to describe \autoref{defn:tp}. What this means is that particles $x\in X$ are observed individually as they are transported to $y\in Y$, and importantly, mass cannot be split, or the flows $T: X\to Y$ must be bijective.\newline From here, we introduce
the Monge formulation, which observes a transport map $T$ that transports $\mu$ to $\nu$. Before we define it, we must define what is meant by transporting one measure to another.
\begin{defn}[Transport Maps]\label{defn:transport_map}
  We say that $T: X\to Y$ transports $\mu\in \mathcal{P}(X)$ to $\nu\in\mathcal{P}(Y)$ if 
  \begin{equation*}\label{eqn:pushforward}
    \mu(T^{-1}(B)) = \nu(B) \quad\text{for all $\nu$-measurable $B\in Y$.}
  \end{equation*}
  This is typically called the \textit{pushforward} of $\mu$ and denoted as $T_\#\mu$.
\end{defn} 
In order to describe the Monge Formulation, we denote the set of all $T$ such that $T_\#\mu=\nu$ with
\begin{equation}
  \mathcal{T}(\mu,\nu) = \left\{T: X\to Y \vert T_\#\mu = \nu,\,\,\text{$T$ measurable} \right\}
\end{equation}
\begin{defn}[Monge Formulation of the transport problem]\label{defn:monge}
  \begin{equation}
    \mathbb{M} = \underset{\mathcal{T}(\mu,\nu)}{\text{minimize}}\left\{\int_X c(x,T(x))d\mu(x)\right\}
  \end{equation}
\end{defn}
In other words, \autoref{defn:monge} is minimizing the energy required from transporting $X\overset{T(x)}{\to}Y$. 
\begin{exmp}[Discrete Monge Problem]\label{exmp:monge}
Consider $\mu = \frac{2}{3}\delta_{x_1} + \frac{1}{3}\delta_{x_2}$ and $\nu = \frac{2}{3}\delta_{y_1} + \frac{1}{3}\delta_{y_2}$. We see that the only transport map that exists is the map where $T(x_1) = y_1$ and $T(x_2)=y_2$ as such mass is not split.
However, in order to make the optimal transport map less trivial, we can allow $\mu = \frac{1}{n}\sum_{i=1}^n\delta_{x_i}$ and $\nu = \frac{1}{n}\sum_{i=1}^n\delta_{y_i}$ to create the discrete Monge optimal transportation problem. \newline
Now, we can allow $j = \sigma(i)$ where $\sigma \in S_n$ with $S_n$ be the permutations of $\{1,2\dots,n\}$. In other words, $\sigma$ is a transport map where we can define the discrete monge problem as 
\[\mathbb{M}_{\text{disc}} = \inf_{\sigma\in S_n}\left\{\frac{1}{n}\sum_{i=1}^nc(x_i,y_{\sigma(i)})\right\}.\]
\begin{figure}[H]
  \includegraphics[scale=0.3]{monge.jpg}
  \caption{We see that $x_1\to y_1$ and $x_2\to y_2$ is the only valid transport map $T$ such that mass is not split. These are solutions to what are discrete optimal transport problems.}
  \label{fig:monge}
\end{figure}
\end{exmp}
\subsection{Kantorovich Formulation}\hfill\\

It's natural to ask what happens on the product space $X\times Y$. The Kantorovich does exactly this by defining \textit{transportation plans} as a product measure on $X\times Y$.

\begin{defn}[Transport Plans]
  A transport plan, $\pi \in \mathcal{P}(X\times Y)$, is valid if all mass taken from a point $x\in X$ must correspond to $d\mu(x)$ and all mass taken from a point $y\in Y$ must correspond to $d\nu(y)$, or in other terms,
  \begin{equation}\label{eqn:const}
    \pi(A\times Y) = \mu(A)\quad \pi(X\times B) = \nu(B).
  \end{equation}
\end{defn}
  
To define  Kantorovich's Formulation, we define the set of all probability measures that follow \autoref{eqn:const} with
\begin{equation}
  \Pi(\mu,\nu) = \left\{\pi\in \mathcal{P}(X\times Y): \text{\autoref{eqn:const} holds for all measurable $A\subset X$, $B\subset Y$}\right\}.
\end{equation}
\begin{defn}[Kantorovich Formulation of the transport problem]\label{defn:kantorovich}
  \begin{equation}
    \mathbb{K} = \underset{\Pi(\mu,\nu)}{\text{minimize}}\left\{\int_{X\times Y} c(x,y)d\pi(x,y)\right\}.
  \end{equation}
\end{defn}
This formulation is typically referred to as a relaxation of \autoref{defn:monge}, or that Monge's problem is stronger than Kantorovich's.
\begin{prop}\label{prop:equiv}
  The Monge Formulation can be written as a Kantorovich Formulation.
\end{prop}
\begin{proof}
  Because mass cannot be split in the Monge Formulation, this means that we can rewrite $d\pi(x,y)$ in Definition \ref{defn:kantorovich}
  with
  \[ d\pi(x,y)=d\pi_T(x,y) \equiv d\mu(x)\delta[y=T(x)]\]
  where $\delta$ is the dirac measure and $T(x)$ is as defined in \autoref{defn:transport_map}. As $\int_Y\xi(x,y)\delta[y=T(x)] = \xi(x,T(x))$ for any nonnegative function $\xi:X\times Y\to \mathbb{R}$, we have that 
  \[ \int_{X\times Y}c(x,y)d\pi(x,y) = \int_X c(x,T(x))d\mu(x).\]
  In order for $\pi_T$ to be in $\Pi(\mu,\nu)$, we consider \autoref{eqn:const}
  \[\pi(A\times Y) = \mu(A)\quad \pi(X \times B) = \nu(B)\]
  so we see that this is the case where we have measurable $B\in Y$ with 
  \[\nu(B) = \mu(T^{-1}(B)).\]
  Of course, we allowed $T(x)$ to follow \autoref{defn:transport_map}, so this condition is satisfied. Furthermore, this is equivalent to the Kantorovich formulation of the transportation problem.
\end{proof}
We see that the Monge Formulation is the stronger case, where we restrict mass in $X$ from being able to be split. However, \autoref{prop:equiv} can also be framed as stating that a Kantorovich Formulation with mass
that cannot be split is a Monge Problem.
\begin{exmp}[Discrete Kantorovich Problem]
  Consider $\mu = \sum_{i=1}^n\alpha_i\delta_{x_i}$ and $\nu = \sum_{i=1}^n\beta_{i}\delta_{y_i}$ where $\sum_{i=1}^n \alpha_i = \sum_{i=1}^n\beta_i = 1$ and $\alpha_i \ge 0$ and $\beta_i \ge 0$. Unlike \autoref{exmp:monge}, $\alpha_i = \beta_i = \frac{1}{n}$ need not be true. Transport plans can be represented as $\pi = \pi_{ij}$\cite{thorpe} where $\pi$ is a $n\times n$ bistochastic matrix in the sense that
  \[B = \left\{\pi \bigg\vert\pi_{ij} \ge 0 \text{ for all $1 \le i,j\le n$}, \quad \sum_{i=1}^n\pi_{ij} = 1 \text{ for all $1\le j\le n$,}\quad \sum_{j=1}^n\pi_{ij} = 1 \text{ for all $1\le i\le n$.}\right\}\]
  So now the discrete Kantorovich problem becomes 
  \begin{equation}
    \mathbb{K}_{\text{disc}} = \inf_{\pi\in B}\sum_{i,j}\pi_{ij}c(x_i,y_j).
  \end{equation}
  \begin{figure}[H]
    \includegraphics[scale=0.3]{discretekant.jpg}
    \caption{Letting $\mu = \frac{2}{3}\delta_{x_1} + \frac{1}{3}\delta_{x_2}$ and $\nu = \frac{2}{3}\delta_{y_1} + \frac{1}{3}\delta_{y_2}$, we see there is no concentration along the diagonal for $\pi$, hence mass is split, but this transport plan is still valid.}
    \label{fig:kant}
  \end{figure}
\end{exmp} 
\section{Kantorovich Duality}

For linear optimization problems, it is well known that they admit a dual problem, and Kantorovich in 1942 showed duality for \autoref{defn:kantorovich} for $c(x,y) = d(x,y)$ where $d$ is the distance metric in $\R^n$, but it still holds in general Polish spaces. 
To begin to understand Kantorovich's dual problem, we can think of Caffarelli's Shipper's Problem described in \cite{villani}.
\begin{exmp}[Shipper's Problem]\label{exmp:ship}
  It costs $c(x_1,y_1)$ dollars for a factory to transport coal from mine $x_1$ to factory $y_1$. I tell the mine owner that I can charge them $\phi(x_1)$ dollars to pick up at mine $x_1$ and charge $\psi(y_1)$ to deliver to factory $y_1$, then, in order for the factory owner to agree to my terms, 
  \[\phi(x)+\psi(y)\le c(x,y)\]
  for every location $x$ and destination $y$. However, I can make this sum as close as I want to $c(x,y)$ because I have the liberty to choose $\phi,\psi$. In other words, the most money I can make is equal to the least effort it would take the factory owner (just simply transporting it with cost $c(x,y)$) which is exactly what a dual problem would suggest.
\end{exmp}
\begin{figure}[H]
  \includegraphics[scale=0.3]{Shippers.jpg}
  \caption{The easiest thing for the owners to do is pay $c(M,F)$, but they can pay less by going through my shipping service which costs $\phi(M) + \psi(F)$.}
\end{figure}
\subsection{Kantorovich Duality}
\begin{thm}[Kantorovich Duality]\label{thm:kant}
Let $X$ and $Y$ be Polish spaces, let $\mu\in \mathcal{P}(X)$ and $\nu\in\mathcal{P}(Y)$, and let\hfill\break $c: X\otimes Y\to \mathbb{R}^+\cup\{+\infty\}$ be a lower semicontinuous cost function.

  Whenever $\pi \in \mathcal{P}(X\otimes Y)$ and $(\phi,\psi)\in L^1(d\mu)\otimes L^1(d\nu)$, define 
  \[I(\pi)= \int_{X\otimes Y}c(x,y)d\pi(x,y),\quad J(\phi,\psi) = \int_X\phi d\mu + \int_Y\psi d\nu.\]
  Define $\Pi(\mu,\nu)$ to be the set of all Borel probability measures $\pi$ on $X\otimes Y$ such that for all measurable subsets $A\subset X$ and $B\subset Y$,
  \[\pi(A\otimes Y) = \mu(A),\quad \pi(X\otimes B)= \nu(B),\]
  and define $\Phi_c$ to be the set of all measurable functions $(\phi,\psi)$ satisfying
  \begin{equation}\label{eqn:dualconst}
    \phi(x) + \psi(y) \le c(x,y)
  \end{equation}
  for $d\mu-$almost every $x\in X$, $d\nu-$almost every $y\in Y$. Then,
  \begin{equation}
    \inf_{\Pi(\mu,\nu)} I(\pi) = \sup_{\Phi_c}{J(\phi,\psi)}.
  \end{equation}
\end{thm}
We can construct a minimax principle in order to give an informal proof of \autoref{thm:kant}. We see 
\begin{equation}\label{eqn:minimax}
  \inf_{\pi\in\Pi(\mu,\nu)} \int_{X\times Y}c(x,y)d\pi = \inf_{\pi\in M_+(X\times Y)} \left\{\int_{X\times Y} c(x,y)d\pi(x,y)+ \left\{\begin{matrix}0 &\pi\in\Pi(\mu,\nu) \\ +\infty &\text{otherwise.}\end{matrix}\right\} \right\}
\end{equation}
where $M_+$ is the set of nonnegative Borel measures on $X\times Y$. We can rewrite the righthand side of \autoref{eqn:minimax} with
\[\left\{\begin{matrix}0 &\pi\in\Pi(\mu,\nu) \\ +\infty &\text{otherwise.}\end{matrix}\right\} = \sup_{(\phi,\psi)\in L^1(d\mu)\otimes L^1(d\nu)}\left\{\int_{X}\phi(x)d\mu + \int_{Y}\psi(y)d\nu- \int_{X\times Y} [\phi(x)+\psi(y)]d\pi(x,y)\right\}.\]
as $\pi\in \Pi$ follows the constraints defined in \autoref{eqn:const}. Now, the Monge-Kantorovich problem can be written as 
\begin{equation}
  \inf_{\pi\in M_+(X\times Y)}\left\{\int_{X\times Y} c(x,y)d\pi(x,y)+\sup_{(\phi,\psi)}\left\{\int_{X}\phi(x)d\mu + \int_{Y}\psi(y)d\nu- \int_{X\times Y} [\phi(x)+\psi(y)]d\pi(x,y)\right\}\right\}.
\end{equation}
Informally exchanging the $\inf$ and $\sup$, we get that the Monge-Kantorovich problem is 
\begin{equation}\label{eqn:swap}
  \sup_{(\phi,\psi)}\left\{\inf_{\pi\in M_+(X\times Y)}\left\{\int_{X\times Y} c(x,y) - [\phi(x) - \psi(y)]d\pi(x,y)\right\}+\int_{X}\phi(x)d\mu + \int_{Y}\psi(y)d\nu\right\}.
\end{equation}
Now, note that 
\[ \inf_{\pi\in M_+(X\times Y)}\left\{\int_{X\times Y} c(x,y) - [\phi(x) - \psi(y)]d\pi(x,y)\right\} = \left\{\begin{matrix}0 &\phi(x) + \psi(y) \le c(x,y) \\ +\infty &\text{otherwise.}\end{matrix}\right\}\]
so the Kantorovich-Monge problem becomes the maximization (dual) problem
\begin{equation}
  \mathbb{K}(\mu,\nu) = \sup_{\Phi_c}\left\{\int_{X}\phi(x)d\mu + \int_{Y}\psi(y)d\nu\right\}.
\end{equation}
We've assumed that $(\phi,\psi) \in Cb(X)\times C_b(Y)$ as $\phi(x)+\psi(y) \le c(x,y)$ holding everywhere is different to holding almost everywhere. Moreover, we need to make this minimax principle rigorous.
\subsection{Fenchel-Rockafellar and Convex Analysis Results}
In order to prove \autoref{thm:kant}, we need a rigorous minimax principle. We start by presenting some results and definitions from convex and functional analysis.
\begin{defn}[Legendre-Fenchel Transform]
  Let $\phi: E \to \R\cup \{+\infty\}$ be convex. Then, the Legendre-Fenchel Transform for $f\in E^\star$ is
  \begin{equation}
    \phi^\star(f) = \sup_{x\in E}\left[\langle f, x\rangle - \phi(x)\right].
  \end{equation}  
\end{defn}
\begin{defn}[Polish Spaces]
A Polish space, $X$, satisfies the following 
\begin{enumerate}
  \item A Borel probability measure $\mu$ on $X$ is regular
  \item A probability measure $\mu$ on $X$ is concentrated on a $\sigma$-compact set.
  \item Any tight family of probability measures in $P(X)$ is relatively sequentially compact in $P(X)$.
  \item Let $X$ be a metric space. If $F$ a nonnegative lower semi-continuous function on $X$ then it can be written as the supremum of an increasing sequence of uniformly continuous nonnegative functions.
  \item If $K$ is a compact metric space then $C(K)$ is separable.
\end{enumerate}
Details of these are presented in \cite{villani} and \cite{billingsey}.
\end{defn}
Now, we introduce the following from \cite{brezis}
\begin{thm}[Fenchel-Rockafellar]\label{thm:Fenchel}
  Let $E$ be a normed vector space and $E^\star$ be its topological dual space. Let $\phi,\psi:E\to\R\cup\{+\infty\}$ be convex. If there is some $x_0\in E$ such that $\phi(x_0),\psi(x_0) <+\infty$ and $\phi$ is continuous at $x_0$, then
  \begin{align*}
    \inf_{x\in E}\{\phi(x)+\psi(x)\} = \sup_{f\in E^\star}\{-\phi^\star(-f)-\psi^\star(f)\} = \max_{f\in E^\star}\{-\phi^\star(-f)-\psi^\star(f)\} = -\min_{f\in E^\star}\{-\phi^\star(-f)-\psi^\star(f)\}
  \end{align*}
  where $\phi^\star(f)$ for $f\in E^\star$ is the \textit{Legendre-Fenchel transform}.
\end{thm}
\begin{proof}
  To be proven. I can base this off of Brezis.
\end{proof}
Moreover, we can check that \autoref{thm:Fenchel} is a minimax principle.
\subsection{Proof of Kantorovich Duality}
Finally, we can begin to prove Kantorovich Duality as stated in \autoref{thm:kant}. The following lemmas come from \cite{villani}.
\begin{lem}
  Under the same conditions as \autoref{thm:kant}, 
  \begin{equation}\label{eqn:leftineq}\sup_{\Phi_C} J(\phi,\psi) \le \inf_{\Pi(\mu,\nu)} I(\pi).\end{equation}
\end{lem}
\begin{proof}
  Let $(\phi,\psi) \in \Phi_c$ and $\pi\in \Pi(\mu,\nu)$. Then,
  \[J(\phi,\psi) = \int_X\phi d\mu + \int_Y\psi d\nu = \int_{X\times Y}[\phi(x)+\psi(y)]d\pi(x,y)\]
  as $\pi$ has marginals $\mu,\nu$.\newline
  Let $N_x,N_y$ be null sets of $\mu$ and $\nu$ respectively. Because \autoref{eqn:dualconst} holds almost everywhere, \autoref{eqn:dualconst} holds for 
  $(x,y)\in N_x^c \times N_y^c$. Moreover, $\pi(N_x\times Y) = \mu(N_x) = 0$ and $\pi(X\times N_y) = \nu(N_y) = 0$ so $\pi((N_x^c\times N_y^c)^c) = 0$ so \autoref{eqn:dualconst} holds for $\pi$. Thus,
  \begin{equation}
    \int_{X\times Y}[\phi(x)+\psi(y)]d\pi(x,y) \le \int_{X\times Y}c(x,y)d\pi(x,y).
  \end{equation}
  \autoref{eqn:leftineq} is recovered upon taking the supremum on the left-hand side and the infimum on the right-hand side.
\end{proof}
\begin{lem}
  Under the same conditions as \autoref{thm:kant}, we have 
  \[\sup_{\Phi_c} J(\phi,\psi) \ge \inf_{\Pi(\mu,\nu)} I(\pi)\]
\end{lem}
\cite{villani} presents the proof in 3 steps of increasing generality:
  \begin{enumerate}[1.]
    \item Assuming $X,Y$ are compact and $c(x,y)$ are continuous.
    \item Keeping the assumption that $c(x,y)$ is continuous, but relaxing compactness of $X,Y$.
    \item Only assuming that $c(x,y)$ is lower semi-continuous.
  \end{enumerate}
  Only 1. will use the minimax principle discussed with \autoref{thm:Fenchel} so the proof will be provided below.
\begin{proof}[Proof of 1.]
  
\end{proof}
\section{Existence and Characterization}
\subsection{Existence of Transport Plans}
\subsection{Existence of Maximizers of the Dual Problem}
\subsection{Existence of Transport Maps}
\section{Wasserstein}
\subsection{Wasserstein Distances}
\subsection{Wasserstein Topology}
\section{Gradient Flows and PDEs}
\subsection{Gradient Flow Motivation}
\subsection{Benamou-Brenier Theorem}


\section{Bibliography}

\begin{thebibliography}{9}
\bibitem{villani}
Villani C.
Topics in Optimal Transportation.
American Mathematical Society, 2003.
\bibitem{thorpe}
Thorpe M.
Introduction to Optimal Transport.
University of Cambridge, 2018.
\bibitem{brezis}
Brezis H.
Functional Analysis, Sobolev Spaces and Partial Differential Equations. 
Springer Press, 2011.
\bibitem{billingsey}
Billingsley P.
Convergence of Probability Measures.
John Wiley \& Sons Inc., 1999.
\end{thebibliography}


\end{document}

